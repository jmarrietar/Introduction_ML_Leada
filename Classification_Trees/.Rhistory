preProc
Mtarix1
Matrix2
clear
data(iris)
library(ggplot2)
names(iris)
View(iris)
names(iris)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
library(caret)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
dim(training)
dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
modFit<-train(Species~.,method="rpart",data=training)
modFit<-train(Species~.,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE,main="Classification Trea")
test(modFit$finalModel,use.n=T,all=T,cex=0.8)
text(modFit$finalModel,use.n=T,all=T,cex=0.8)
text(modFit$finalModel,use.n=T,all=T,cex=0.5)
plot(modFit$finalModel,uniform=TRUE,main="Classification Trea")
text(modFit$finalModel,use.n=T,all=T,cex=0.5)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone,package="ElemStatLearn")
head(ozone)
library(ElemStatLearn)
data(prostate)
str(prostate)
small=prostate[1:5,]
lm(lpsa~.,data=Small)
small=prostate[1:5,]
lm(lpsa~.,data=Small)
lm(lpsa~.,data=small)
library(ISLR)
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
inBuild<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
validation<-Wage[-inBuild,]
buildData<-Wage[inBuild,]
inTrain<-createDataPartition(y=buildData$wage,p=0.7,list=FALSE)
training<-buildData[inTrain,]
testing<-buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
mod1<-train(wage~.,method="glm",data=training)
mod2<-train(wage~.,method="rf",data=training,trControl=trainControl(method="cv"),number=3)
pred1<-predict(mod1,testing)
pred2<-predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
predDF<-data.frame(pred1,pred2,wage=testing$wage)
View(predDF)
combModFit<-train(wage~.,method="gam",data=predDF)
combModFit<-train(wage~.,method="gam",data=predDF)
combPred<-predict(combModFit,predDF)
combPred
sqrt(sum(pred1-testing$wage)^2)
sqrt(sum(pred2-testing$wage)^2)
sqrt(sum(combPred-testing$wage)^2)
pred1V<-predict(mod1,validation)
pred2V<-predict(mod2,validation)
predVDF<-data.frame(pred1=pred1V,pred2=pred2V)
predVDF
predVDF<-data.frame(pred1=pred1V,pred2=pred2V)
sqrt(sum(pred1V-validation$wage)^2)
sqrt(sum(pred2V-validation$wage)^2)
sqrt(sum(combPredV-validation$wage)^2)
combPredV
sqrt(sum(pred1V-validation$wage)^2)
sqrt(sum(pred2V-validation$wage)^2)
combPredV<-data.frame(pred1=pred1V,pred2=pred2V)
View(combPredV)
sqrt(sum(combPredV-validation$wage)^2)
sqrt(sum((pred1V-validation$wage)^2))
library(quantmod)
from.dat<-as.Date("01/01/08",format="%m/%d/%y")
to.dat<-as.Date("12/31/13",format="%m/%d/%y")
library(quantmod)
library(quantmod)
install.packages("quantmod")
install.packages("quantmod")
library(quantmod)
install.packages("quantmod")
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
library(quantmod)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
head(GOOG)
mGoog<-to.monthly(GOOG)
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
library(quantmod)
from.dat<-as.Date("01/01/08",format="%m/%d/%y")
to.dat<-as.Date("12/31/13",format="%m/%d/%y")
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
head(GOOG)
mGoog<-to.monthly(GOOG)
from.dat<-as.Date("01/01/08",format="%m/%d/%y")
to.dat<-as.Date("12/31/13",format="%m/%d/%y")
getSymbols("GOOG",src="google",from=from.dat,to=to.dat)
#Summarize monthly and store as time series.
mGoog<-to.monthly(GOOG)
mGoog<-to.monthly(GOOG)
googOpen<-Op(mGoog)
tsl<-ts(googOpen,frequency=12)
plot(tsl,xlab="Years+1",ylab="GOOG")
kMeans<-kmeans(subset(training,select=-c(Species)),center=3)
kMeans<-kmeans(subset(training,select=-c(Species)),centers=3)
training
View(testing)
data(iris)
library(ggplot2)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
dim(training)
dim(testing)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
data(iris)
library(ggplot2)
library(caret)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
dim(training)
dim(testing)
kMeans1<-kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters<-as.factor(Kmeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
kMeans1<-kmeans(subset(training,select=-c(Species)),centers=3)
kMeans1
training$clusters<-as.factor(kmeans1$cluster)
training$clusters<-as.factor(kMeans1$cluster)
View(training)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
table(kMeans1$cluster,training$Species)
modFit<-train(clusters~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)
testClusterPred<-predict(modFit,testing)
table(testClusterPred,testing$Species)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
vowel.test
View(vowel.test)
View(vowel.train)
str(vowel.train)
vowel.train$y<-as.factor(vowel.train$y)
str(vowel.train)
vowel.train$y<-as.factor(vowel.train$y)
vowel.test$y<-as.factor(vowel.test$y)
str(vowel.train)
str(vowel.test)
modFit<-train(y~.,data=vowel.train,method="rf",prox=TRUE)
modFit
library(caret)
modFit<-train(y~.,data=vowel.train,method="rf",prox=TRUE)
modFit
modFit2<-train(wage~.,method="gbm",data=vowel.train,verbose=FALSE)
print(modFit2)
modFit2<-train(y~.,method="gbm",data=vowel.train,verbose=FALSE)
modFit
pred1<-predict(modFit,vowel.test)
pred1
vowel.test$predRight<-pred1==vowel.test$y
View(vowel.train)
View(vowel.test)
table(pred1,vowel.test$y)
(1+1)
Tabla1<-table(pred1,vowel.test$y)
confusionMatrix(Tabla1)
data(vowel.train)
data(vowel.test)
vowel.train$y<-as.factor(vowel.train$y)
vowel.test$y<-as.factor(vowel.test$y)
str(vowel.train)
str(vowel.test)
# Fit (1) a random forest predictor relating the factor variable y to the remaining variables
set.seed(33833)
modFit<-train(y~.,data=vowel.train,method="rf",prox=TRUE)
modFit
#a boosted predictor using the "gbm" method.
#Fit the model. #gbm<- Boosting with Trees
modFit2<-train(y~.,method="gbm",data=vowel.train,verbose=FALSE)
print(modFit2)
#What are the accuracies for the two approaches on the test data set?
#Predict New Values Random forest
pred1<-predict(modFit,vowel.test)
vowel.test$predRight<-pred1==vowel.test$y
Tabla1<-table(pred1,vowel.test$y)
confusionMatrix(Tabla1)
pred2<-predict(modFit2,vowel.test)
Tabla2<-table(pred2,vowel.test$y)
confusionMatrix(Tabla2)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
View(testing)
mod1<-train(diagnosis~.,method="glm",data=training)
mod2<-train(diagnosis~.,method="rf",data=training,trControl=trainControl(method="cv"),number=3)
modlda=train(diagnosis~.,data=training,method="lda")
pred1<-predict(mod1,testing)
pred2<-predict(mod2,testing)
pred3<-predict(modlda,testing)
pred3
predDF<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
View(predDF)
View(predDF)
combModFit<-train(diagnosis~.,method="rf",data=predDF,trControl=trainControl(method="cv"),number=3)
combPred<-predict(combModFit,predDF)
combPred
#Accuracy
Tabla3<-table(combPred,testing$diagnosis)
confusionMatrix(Tabla3)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
View(concrete)
View(mixtures)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
View(concrete)
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
View(testing)
View(training)
View(concrete)
colnames(testing)
testing[,9]
svm<-predict(svm.model,testing[,-9])
svm
sqrt(sum((svm-testing[,9])^2))
#Evaluate on testing
sqrt((sum((svm-testing[,9])^2))/256)
sqrt((sum((svm-testing[,9])))/256)
sqrt((sum((svm-testing[,9])^2))/256)
sqrt((sum((svm-testing[,9])^2)))
107.44/2
107.44/256
sqrt((sum((svm-testing[,9])^2)))
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(325)
library(e1071)
#svm
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
colnames(testing)
#Evaluate on testing
sqrt((sum((svm-testing[,9])^2))/256)
sqrt((sum((svm-testing[,9])^2))/256)
#Question 5
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
#svm
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
colnames(testing)
#Evaluate on testing
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
#svm
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(3523)
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
colnames(testing)
#Evaluate on testing
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(325)
svm.model<-svm(CompressiveStrength~.,data=training,cost=100,gamma=1)
svm<-predict(svm.model,testing[,-9])
colnames(testing)
#Evaluate on testing
sqrt((sum((svm-testing[,9])^2))/256)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm.model<-svm(CompressiveStrength~.,data=training)
svm<-predict(svm.model,testing[,-9])
sqrt((sum((svm-testing[,9])^2))/256)
## Conclusion
setwd("C:/Users/Alfonso/Desktop/JOM/AB-Testing")
Delivery_Data<-read.csv("Delivery_Data_problem.csv",header=T,)
View(Delivery_Data)
sort(table(Safeway$pickup_zipcode))
Delivery_Data<-read.csv("Delivery_Data_problem.csv",header=T,)
sort(table(Delivery_Data$pickup_name))
Safeway<-subset(Delivery_Data, pickup_name=="Safeway")
sort(table(Safeway$pickup_zipcode))
Whole_Foods_Market<-subset(Delivery_Data, pickup_name=="Whole Foods Market")
sort(table(Whole_Foods_Market$pickup_zipcode))
sort(table(Whole_Foods_Market$pickup_zipcode))
sort(table(Delivery_Data$pickup_name))
Pizzeria_Delfina<-subset(Delivery_Data, pickup_name=="Pizzeria Delfina")
sort(table(Pizzeria_Delfina$pickup_zipcode))
Delivery_Data$pickup_name<-gsub('Safeway','Safeway-H1',Delivery_Data$pickup_name,fixed=TRUE)
View(Delivery_Data)
Delivery_Data$pickup_name<-gsub('Safeway','Whole Foods Market-H2',Delivery_Data$pickup_name,fixed=TRUE)
Delivery_Data$pickup_name<-gsub('Safeway','Pizzeria Delfina-H3',Delivery_Data$pickup_name,fixed=TRUE)
sort(table(Delivery_Data$pickup_name))
Delivery_Data<-read.csv("Delivery_Data_problem.csv",header=T,)
#Using gsub() functions.
Delivery_Data$pickup_name<-gsub('Safeway','Safeway-H1',Delivery_Data$pickup_name,fixed=TRUE)
Delivery_Data$pickup_name<-gsub('Whole Foods Market','Whole Foods Market-H2',Delivery_Data$pickup_name,fixed=TRUE)
Delivery_Data$pickup_name<-gsub('Pizzeria Delfina','Pizzeria Delfina-H3',Delivery_Data$pickup_name,fixed=TRUE)
#What is the most popular business in terms of delivery frequency?
sort(table(Delivery_Data$pickup_name))
grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name)
count(grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name))
sum(grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name))
#What is the total number of deliveries identified as coming from a 'hotspot' labeled business?
Delivery_Data$pickup_name(grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name))
Delivery_Data$pickup_name(grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name))
Delivery_Data$pickup_name[grep('[[:alpha:]]+\\-',Delivery_Data$pickup_name)]
#What is the total number of deliveries identified as coming from a 'hotspot' labeled business?
Delivery_Data$pickup_name[grep('[[:alpha:]]+\\-+[H]+[[:digit:]]',Delivery_Data$pickup_name)]
grep('[[:alpha:]]+\\-+[H]+[[:digit:]]',Delivery_Data$pickup_name)
Delivery_Data$pickup_name[grep('[[:alpha:]]+\\-+[H]+[[:digit:]]',Delivery_Data$pickup_name)]
str(Delivery_Data$pickup_name)
str(Delivery_Data)
str(date_created_local)
str(Delivery_Data$date_created_local)
mydate <- factor("2014-03-01 00:01:44.538420-08:00")
#What is the total number of deliveries identified as coming from a 'hotspot' labeled business?
Delivery_Data$pickup_name[grep('[[:alpha:]]+\\-+[H]+[[:digit:]]',Delivery_Data$pickup_name)]
sum(data$AF50CR)
data$AF50CR<-0
identified<-which(data$landmass==1 && data$population>20 && data$religion==1)
data$AF50CR[identified]=1
sum(data$AF50CR)
identified<-which(data$landmass==1 && data$population>20 && data$religion==1)
data$AM20CHR<-0
setwd("C:/Users/Alfonso/Desktop/JOM/Introduction_Machine_Learnin_TeamLeada/Data_Wrangling_II")
#Data: Flag DataBase from UC irvine Machine Learning database.
data<-read.csv('flag_data.csv',header=T,stringsAsFactors=F)
data$AM20CHR<-0
identified<-which(data$landmass==1 && data$population>20 && data$religion==1)
identified<-which(data$landmass==(1||2) && data$population>20 && data$religion==1)
identified<-which(data$landmass==(1||2) && data$population>20 && data$religion==1)
identified<-which(data$landmass==(2) && data$population>20 && data$religion==1)
identified<-which(data$landmass==2 && data$population>20 && data$religion==1)
identified<-which(data$landmass==1 && data$population>20 && data$religion==1)
data$AM20CHR[identified]=1
sum(data$AM20CHR)
View(data)
data$landmass==1
View(data)
data$population>20
data$religion==1
View(data)
data$landmass<-gsub('Asia',5,data$landmass,fixed=TRUE)
data$landmass<-gsub('N.America',1,data$landmass,fixed=TRUE)
data$landmass<-gsub('S.America',2,data$landmass,fixed=TRUE)
data$landmass<-gsub('Europe',3,data$landmass,fixed=TRUE)
data$landmass<-gsub('Africa',4,data$landmass,fixed=TRUE)
data$landmass<-gsub('Oceania',6,data$landmass,fixed=TRUE)
identified<-which(data$landmass==1 && data$population>20 && data$religion==1)
View(data)
data$landmass==1
identified<-which(data$landmass==2 && data$population>20 && data$religion==1)
identified
data$landmass==2
identified<-which(data$landmass==2 & data$population>20 & data$religion==1)
identified <- which(data$landmass == 4 & data$population > 5 & data$religion == 1)
identified <- which(data$landmass == 4 & data$population > 20 & data$religion == 1)
identified <- which(data$landmass == 1 & data$population > 20 & data$religion == 1)
identified <- which(data$landmass == 2 & data$population > 20 & data$religion == 1)
identified <- which(data$landmass ==(1||2) & data$population > 20 & data$religion == 1)
identified <- which((data$landmass ==1 ||data$landmass ==2)  & data$population > 20 & data$religion == 1)
data$landmass ==1
data$landmass ==2)
(data$landmass ==1 ||data$landmass ==2)
identified <- which((data$landmass ==2 & data$population > 20 & data$religion == 1)
data$AM20CHR[identified]=1
sum(data$AM20CHR)
identified <- which((data$landmass ==1 & data$population > 20 & data$religion == 1)
)
data$AM20CHR[identified]=1
sum(data$AM20CHR)
sum(data$AM20CHR[identified]$population)
data[which(data$AM20CHR==1)]
which(data$AM20CHR==1)
which(data$AM20CHR==1)
data[unos]
data[,unos]
data[unos,]
unos
unos<-which(data$AM20CHR==1)
data[unos,]
data[unos]
data[unos,]
data[unos,]$population
sum(data[unos,]$population)
setwd("C:/Users/Alfonso/Desktop/JOM/Introduction_Machine_Learnin_TeamLeada")
data<-read.csv('flag_data.csv',header=T,stringsAsFactors=F)
setwd("C:/Users/Alfonso/Desktop/JOM/Introduction_Machine_Learnin_TeamLeada")
#Data-> Flag data taken from UC Irvine Machine Learning database.
data<-read.csv('flag_data.csv',header=T,stringsAsFactors=F)
setwd("C:/Users/Alfonso/Desktop/JOM/Introduction_Machine_Learnin_TeamLeada/Classification_Trees")
##############################
#Supervised Machine Learning##
##############################
#Data-> Flag data taken from UC Irvine Machine Learning database.
data<-read.csv('flag_data.csv',header=T,stringsAsFactors=F)
data<-read.csv('flag_data.csv',header=T)
traindata <- data[1:150, ]
testdata <- data[151:194, -6]
install.packages('rpart')
library('rpart')
model<-rpart(religion~circles+crosses+saltires+quarters+sunstars+crescent+triangle,data=traindata,method='class',control=rpart.control(minsplit=10))
quiz_model<-rpart(religion~landmass+population+language,method='class',control=rpart.control(minsplit=10))
model<-rpart(religion~circles+crosses+saltires+quarters+sunstars+crescent+triangle,data=traindata,method='class',control=rpart.control(minsplit=10))
quiz_model<-rpart(religion~landmass+population+language,data='traindata,'method='class',control=rpart.control(minsplit=10))
quiz_model<-rpart(religion~landmass+population+language,data=traindata,'method='class',control=rpart.control(minsplit=10))
quiz_model<-rpart(religion~landmass+population+language,data=traindata,method='class',control=rpart.control(minsplit=10))
View(traindata)
quiz_model<-rpart(religion~landmass+population+language,data=traindata,method='class')
model
plot(model)
text(model)
subset<-traindata[1:10,-7]
train_subset<-traindata[11:150,]
subset_religions<-data[1:10,7]
cv_model<-rpart(religio~circles+crosses+saltires+quarters+sunstars+crescent+triangle,data=train_subset,method='class',control=rpart.control(minsplit=10))
cv_model<-rpart(religion~circles+crosses+saltires+quarters+sunstars+crescent+triangle,data=train_subset,method='class',control=rpart.control(minsplit=10))
cv_predictions<-predict(cv_model,newdata=subset,type='class')
cv_predictions<-predict(cv_model,newdata=subset,type='class')
cv_predictions
subset
cv_predictions<-as.vector(cv_predictions)
cv_predictions
subset_religion
subset_religions<-data[1:10,7]
subset_religion
subset_religions
cv_predcitions==subset_religions
cv_predictions==subset_religions
sum(cv_predictions==subset_religions)
cv_predictions
subset_religions
subset_religions2<-traindata[11:20, 7]
cv_predictions2<-predict(cv_model,newdata=subset_two,type='class')
subset_two <- traindata[11:20, ]
subset_religions2<-traindata[11:20, 7]
cv_model<-rpart(religion~circles+crosses+saltires+quarters+sunstars+crescent+triangle,data=train_subset,method='class',control=rpart.control(minsplit=10))
cv_predictions2<-predict(cv_model,newdata=subset_two,type='class')
cv_predictions_two<-predict(cv_model,newdata=subset_two,type='class')
cv_predictions_two<-as.vector(cv_predictions_two)
sum(cv_predictions_two==subset_religions_two)
sum(cv_predictions_two==subset_religions2)
